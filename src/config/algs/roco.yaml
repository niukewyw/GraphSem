action_selector: "epsilon_greedy"
epsilon_start: 1.0
epsilon_finish: 0.06
evaluation_epsilon: 0.05
runner: "episode"
buffer_size: 5000
epsilon_anneal_time: 60000
epsilon_anneal_time_exp: 60000


# update the target network every {} episodes
target_update_interval_or_tau: 200
obs_agent_id: True
obs_last_action: False
obs_individual_obs: False
standardise_returns: False
standardise_rewards: True

mac: "roco_mac"
agent: "roco"
agent_output_type: "q"
learner: "roco_learner"
double_q: True
mixer: "qmix"
use_rnn: True
mixing_embed_dim: 32
hypernet_layers: 2
hypernet_embed: 64

encoder_use_rnn: True
encoder_hidden_dim: 32
ae_enc_hidden_dims: []
ae_dec_hidden_dims: []
noise_env: False
noise_type: 0 

attn_embed_dim: 16
concat_obs: True
state_repre_dim: 8
action_embed_dim: 8
state_encoder: ob_attn_ae
ob_embed_dim: 32
momentum_tau: 1

# configs about whether to use latent model learning
use_latent_model: True
use_rew_pred: True
use_momentum_encoder: True
use_residual: True
pred_len: 2
latent_model: mlp
model_hidden_dim: 64
spr_dim: 32
rl_signal: True

# config for trade-off among different losses
spr_coef: 1
rew_pred_coef: 1
repr_coef: 1
minpts: 1

name: "roco"

